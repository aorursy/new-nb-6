import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
print(os.listdir("../input"))
import json
from pandas.io.json import json_normalize
import matplotlib.pyplot as plt
import seaborn as sns
import lightgbm as lgb
import lightgbm as lgb
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
import time

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity='all'
def load_dataset(path,nrows=None):
    cols=['channelGrouping', 'date', 'device', 'fullVisitorId', 'geoNetwork',
        'socialEngagementType', 'totals', 'trafficSource', 'visitId','customDimensions',
        'visitNumber', 'visitStartTime']
    json_cols=['device','geoNetwork', 'totals', 'trafficSource']
    df = pd.read_csv(path, 
                     converters={column: json.loads for column in json_cols}, 
                     dtype={'fullVisitorId': 'str'}, # Important!!
                     nrows=nrows, usecols=cols)
    for column in json_cols:
        column_as_df = json_normalize(df[column])
        column_as_df.columns = [f"{column}.{subcolumn}" for subcolumn in column_as_df.columns]
        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)
    return df
train=load_dataset('../input/train_v2.csv')
test=load_dataset('../input/test_v2.csv')
#Understanding the data and features in the test and train 
#We can see that in all there are 59 features in training_set, while test set has 58 features with 'trafficSource.campaignCode' missing in it.    
train.shape
test.shape

train.head(5)
test.head(5)

train.columns.difference(test.columns)
test.columns.difference(train.columns)
train.dtypes

del train['trafficSource.campaignCode']
data.head(5)
data.columns
# if we see the features below has only one distinct value, which can't be useful for training the model, hence dropping the columns.
for cols in train.columns:
    if(train[cols].nunique() == 1):
        train[cols].value_counts()
        del train[cols]

for cols in test.columns:
    if(test[cols].nunique() == 1):
        del test[cols]
        
#Assigning the values'Train' and 'Test' to train and test data.
train['set']='Train'
test['set']='Test'

# Combining the train and test set for analyses, proprocessing and exploratory data analysis and then dividing back as given
data= pd.concat([train,test],axis=0)
data.drop('customDimensions',inplace=True,axis=1)
data.head(5)
check=pd.to_datetime(data['date'])
dates=data['date'].apply(lambda x: pd.to_datetime(str(x),unit='s'))
                    
hr=dates.dt.hour
#hr.value_counts()
mint=dates.dt.minute
#mint.value_counts().plot(kind='bar',color='orange')
sec=dates.dt.second
#sec.value_counts()
#dates.value_counts()
hr_year= pd.pivot_table(data=data,index=['year'])
#finding the percentage of missing values in columns
for col in data.columns:
    if data[col].isnull().sum() > 0:
        rate = data[col].isnull().sum() * 100 / data.shape[0]
        print(f'Column {col} has {rate:.4f}% missing values.')
    if data[col].dtype == 'object':
        if (data[col] == 'not available in demo dataset').sum() > 0:
            rate = (data[col] == 'not available in demo dataset').sum() * 100 / data.shape[0]
            print(f'Column {col} has {rate:.4f}% values not available in dataset.')
# the data type of the date variable is object, changing it to date. Lets us break the date in Day, Date, Weeks and check for the revenue generated
import datetime
data['date']=data['date'].apply(lambda x: pd.to_datetime(str(x),format='%Y%m%d'))
data['year']=data['date'].dt.year
data['month']=data['date'].dt.month
data['day']=data['date'].dt.day
data['weekday']=data['date'].dt.weekday
data.head(4)
#Analysing the response variable
# we can see that the target variable, almost 98.9 percent of values are null. i.e. the distribution of the data is imbalanced
not_null=pd.notnull(data['totals.transactionRevenue']).sum()
print(f'No of records generating the transactions : {not_null}')
null_val=round(pd.isnull(data['totals.transactionRevenue']).sum()/data.shape[0],4)
print(f'No of records with Null transaction : {null_val}')

data['totals.transactionRevenue']=data['totals.transactionRevenue'].fillna('0')
plt.subplots_adjust(wspace=2)
plt.figure(figsize=(15,5))
#Most of the search for gstore are organic search and Social search.
plt.subplot(1,2,1)
data['channelGrouping'].value_counts().plot(kind='bar',color='orange')

# We can see that the referral channel, has more total revenew
# if we compare both the graphs, we can see the number of visits through channel 'Organic search' and ' Social' are high, but the revenue generated by them is less
# The average revenuw generated from Referral is high
data['totals.transactionRevenue']=data['totals.transactionRevenue'].astype('int')
plt.subplot(1,2,2)
channelwise_renew= pd.pivot_table(data=data,index='channelGrouping',values='totals.transactionRevenue',aggfunc='mean').reset_index()
channelwise_renew=channelwise_renew.sort_values(by='totals.transactionRevenue',ascending=False)
sns.barplot(data=channelwise_renew,x='channelGrouping',y='totals.transactionRevenue',color='orange')
plt.xticks(rotation=90)
plt.show();

# From the below graphs, we can see visualize the percentage of the Channel, generating the revenue
plt.subplots_adjust(wspace=2)
plt.figure(figsize=(15,5))
renew_count=data[data['totals.transactionRevenue']!=0]['channelGrouping'].value_counts().sort_index()
no_renew_count=data[data['totals.transactionRevenue']==0]['channelGrouping'].value_counts().sort_index()
no_renew_count=no_renew_count.rename(cols=['index','NoTrans'])
reve_cnt= pd.concat([renew_count,no_renew_count],axis=1)
reve_cnt=reve_cnt.rename(columns={'channelGrouping':'Revenue',0:'NoRevenue'}).apply(lambda x: x/x.sum()*100,axis=1)
reve_cnt=reve_cnt.reset_index()
plt.subplot(1,3,1)
sns.barplot(data=reve_cnt,x='index',y='NoRevenue',color='orange')
plt.xticks(rotation=90)
plt.xlabel('Channel')
plt.subplot(1,3,2)
sns.barplot(data=reve_cnt,x='index',y='Revenue',color='red')
plt.xticks(rotation=90)
plt.xlabel('Channel')
plt.subplot(1,3,3)
sns.barplot(data=reve_cnt,x='index',y='NoRevenue',color='orange')
sns.barplot(data=reve_cnt,x='index',y='Revenue',color='red')
plt.xticks(rotation=90)
plt.xlabel('Channel')
plt.show();

#checking the revenue growth yearwise, we can see that revenue has been decreased from 2016 to 2018 and the same goes with the channels.
plt.subplots_adjust(wspace=2)
plt.figure(figsize=(15,8))
plt.subplot(1,2,1)
data.groupby('year')['totals.transactionRevenue'].mean().plot(kind='bar',color='orange')
plt.subplot(1,2,2)
year_revenew=pd.pivot_table(data=data,index=['year','channelGrouping'],values='totals.transactionRevenue',aggfunc='mean').reset_index()
sns.barplot(data=year_revenew,x='year',y='totals.transactionRevenue',hue='channelGrouping')
plt.show();
# We can see the spikes in the total revenuw generated and could also see that bigger transaction amount have increased the variance in some months
# If we see the revenue generated in 2016 and 2017, there can be pattern where the revenue is gradually decreasing from august till november and then slowly 
# increasing
plt.subplots_adjust(wspace=2)
plt.figure(figsize=(15,8))
plt.subplot(1,2,1)
year_revenew=pd.pivot_table(data=data,index=['month','year'],values='totals.transactionRevenue',aggfunc='mean').reset_index()
sns.barplot(data=year_revenew,x='month',y='totals.transactionRevenue',color='orange')
ax=plt.subplot(1,2,2)
sns.lineplot(data=year_revenew,x='month',y='totals.transactionRevenue',hue='year',markers=True, style='year')
ax.grid(b=True, which='major', color='w', linewidth=1.0)
plt.xticks(range(0,13))
plt.show();
#Deleting the repeated columsn
# We can see that there are spikes in April'2017, and later in July'2018, may be due to some offers, need to check
plt.figure(figsize=(15,8))
asp=data.groupby('date')['totals.transactionRevenue'].mean().plot()
year_revenew=pd.pivot_table(data=data,index='weekday',values='totals.transactionRevenue',aggfunc='mean').reset_index()
sns.lineplot(data=year_revenew,x='weekday',y='totals.transactionRevenue')
plt.show();
# deleting columns with unique values
data.columns
del data['visitId']
#del data['visitStartTime'] -----can be usefull
del data['visitNumber']
#We can see that most of the browser are not specified as the cookies are disabled . 
# Firefox and Chrome, brings out most the total revenue, this can help us to increase the revenue by concentrating on this browser.
#data['device.browser'].value_counts()

browz=data.groupby('device.browser')['totals.transactionRevenue'].mean().sort_index()
browz=pd.DataFrame(browz)
browz[browz['totals.transactionRevenue']>0].sort_values(ascending=False, by='totals.transactionRevenue').plot(kind='bar')
#data['trafficSource.source'].value_counts()
data.head(5)
# If we see with the total revenue generated most of it comes from Desktop,but we can see that there are outliers in revenue amount
# Grouping the categories, channel and revenue, most of the revenue is generated through the display accessed through the desktop application
plt.subplots_adjust(wspace=2)
plt.figure(figsize=(15,6))
chanel_device=pd.pivot_table(data=data,index=['channelGrouping','device.deviceCategory'],values='totals.transactionRevenue',aggfunc='mean').reset_index()
plt.subplot(1,2,1)
sns.barplot(data=chanel_device,x='device.deviceCategory',y='totals.transactionRevenue')
plt.xlabel('DeviceCategory')
plt.subplot(1,2,2)
data['device.deviceCategory'].value_counts()
sns.barplot(data=chanel_device,x='channelGrouping',y='totals.transactionRevenue',hue='device.deviceCategory')
plt.xticks(rotation=90)
plt.show();
# Total revenue generated through the mobile is low as compared to the Desktop
data['device.isMobile'].value_counts()
round(data.groupby('device.isMobile')['totals.transactionRevenue'].mean(),2).plot(kind='bar')
# We can see that total revenue generated by operating system through which the Gstore is accessed is 'Windows' and 'MAC'.
# But there is variance and outliers, that can be seen in blow graphs.

plt.subplots_adjust(hspace=5)
plt.figure(figsize=(20,6))
plt.subplot(1,3,1)
data['device.operatingSystem'].value_counts().plot(kind='bar',color='orange')
plt.subplot(1,3,2)
op_device=pd.pivot_table(data=data,index=['device.deviceCategory','device.operatingSystem','channelGrouping'],values='totals.transactionRevenue',aggfunc='mean').reset_index()
op_device1=op_device[op_device['totals.transactionRevenue']>0]
sns.barplot(data=op_device1,x='device.operatingSystem',y='totals.transactionRevenue',hue='device.deviceCategory',dodge=False)
plt.xticks(rotation=90)
plt.subplot(1,3,3)
sns.barplot(data=op_device1,x='device.operatingSystem',y='totals.transactionRevenue',hue='channelGrouping',dodge=False)
plt.legend(loc='upper left')
plt.xticks(rotation=90)
plt.show();
# Checking the revenue generated city wise
#data['geoNetwork.city'].value_counts()
plt.figure(figsize=(24,6))
city_rev=pd.DataFrame(data.groupby('geoNetwork.city')['totals.transactionRevenue'].mean()).reset_index()
city_rev1=city_rev[city_rev['totals.transactionRevenue']>0].sort_values(by='totals.transactionRevenue',ascending=False)
sns.barplot(data=city_rev1.head(50),x='geoNetwork.city',y='totals.transactionRevenue',color='orange')
plt.xticks(rotation=90)
plt.show();
# average revenue comes from Americas
data.columns
plt.subplots_adjust(hspace=5)
plt.figure(figsize=(15,6))
plt.subplot(1,2,1)
#data.groupby(['geoNetwork.country','geoNetwork.city'])['totals.transactionRevenue'].mean()
data.groupby('geoNetwork.continent')['totals.transactionRevenue'].mean().plot(kind='bar',color='orange')
plt.subplot(1,2,2)
country_rev=pd.DataFrame(data.groupby('geoNetwork.country')['totals.transactionRevenue'].mean()).reset_index()
country_rev=country_rev[country_rev['totals.transactionRevenue']>0].sort_values(by='totals.transactionRevenue',ascending=False)
sns.barplot(data=country_rev.head(15),x='geoNetwork.country',y='totals.transactionRevenue', color='orange')
plt.xticks(rotation=90)
plt.show();
#data.columns
#data['geoNetwork.metro'].value_counts()
plt.figure(figsize=(15,6))
metro_rev=pd.DataFrame(data.groupby('geoNetwork.metro')['totals.transactionRevenue'].mean()).reset_index()
metro_rev=metro_rev[metro_rev['totals.transactionRevenue']>0].sort_values(by='totals.transactionRevenue',ascending=False)
sns.barplot(data=metro_rev.head(30),x='geoNetwork.metro',y='totals.transactionRevenue',color='orange')
plt.xticks(rotation=90)
plt.show();
#data.columns
#data['geoNetwork.region'].value_counts()
plt.figure(figsize=(15,6))
net_reg=pd.pivot_table(data=data, index=['geoNetwork.region'],values='totals.transactionRevenue',aggfunc='mean').reset_index()
net_reg=net_reg[net_reg['totals.transactionRevenue']>0].sort_values(by='totals.transactionRevenue',ascending=False)
sns.barplot(data=net_reg.head(40),x='geoNetwork.region',y='totals.transactionRevenue',color='orange')
plt.xticks(rotation=90)
plt.show();
data['totals.hits']=data['totals.hits'].fillna(1) 
data['totals.hits']=data['totals.hits'].astype('Int64')
# We can see that some relation coming out from hits and total revenue, if the hits are more, there is high chances transaction
plt.figure(figsize=(10,5))
#data['totals.hits'].value_counts()
#data['totals.hits'].isnull().sum()
hits_rev=pd.pivot_table(data=data,index='totals.hits',values='totals.transactionRevenue',aggfunc='mean')
hits_rev=hits_rev.reset_index().sort_values(by='totals.hits',ascending=False)
hits_rev1=hits_rev[hits_rev['totals.transactionRevenue']>0].sort_values(by='totals.hits',ascending=False)
sns.scatterplot(data=hits_rev1,x='totals.hits',y='totals.transactionRevenue')
plt.xticks(rotation=90)
plt.ylim(-10,1.683176e+08)
plt.show();
hits_rev['totals.transactionRevenue'].min()
hits_rev['totals.transactionRevenue'].max()
hits_rev.quantile([0.0,0.50,0.75,0.9])
#filling by1
data['totals.pageviews']=data['totals.pageviews'].fillna(1) 
data['totals.pageviews']=data['totals.pageviews'].astype('Int64')
data['totals.pageviews'].dtypes

# We can also see that with the increase in the pageview, the transactionRevenue also increases.
plt.figure(figsize=(10,5))
#data.columns
#data['totals.pageviews'].value_counts()
page_rev=pd.pivot_table(data=data,index='totals.pageviews',values='totals.transactionRevenue',aggfunc='mean')
page_rev=page_rev.reset_index().sort_values(by='totals.pageviews',ascending=False)
#hits_rev1=hits_rev[hits_rev['totals.transactionRevenue']>0].sort_values(by='totals.hits',ascending=False)
sns.scatterplot(data=page_rev,x='totals.pageviews',y='totals.transactionRevenue')
plt.xticks(rotation=90)
plt.ylim(-10,1.683176e+08)
plt.show();
# Higher the sessionQualityDim, there are high chances transaction and generate the revenue
data['totals.sessionQualityDim']= data['totals.sessionQualityDim'].fillna(0)
data['totals.sessionQualityDim']=data['totals.sessionQualityDim'].astype('Int64')
#data.groupby('totals.sessionQualityDim')['totals.transactionRevenue'].mean()
plt.figure(figsize=(10,5))
session_rev= pd.pivot_table(data=data,index='totals.sessionQualityDim',values='totals.transactionRevenue',aggfunc='mean').reset_index()
#session_rev
sns.scatterplot(data=session_rev,x='totals.sessionQualityDim',y='totals.transactionRevenue')
#fillinh NA values with mode 5
#data['totals.timeOnSite'].isnull().sum()
data['totals.timeOnSite']=data['totals.timeOnSite'].fillna('5')
data['totals.timeOnSite']=data['totals.timeOnSite'].astype('Int64')
plt.figure(figsize=(10,5))
site_rev= pd.pivot_table(data=data,index='totals.timeOnSite',values='totals.transactionRevenue',aggfunc='mean').reset_index()
sns.scatterplot(data=site_rev,x='totals.timeOnSite',y='totals.transactionRevenue')
# Total Transaction revenue is generated through the Referral.
data['totals.totalTransactionRevenue']=data['totals.totalTransactionRevenue'].fillna(0)
data['totals.totalTransactionRevenue']=data['totals.totalTransactionRevenue'].astype('Int64')
data.groupby('channelGrouping')['totals.totalTransactionRevenue'].mean().plot(kind='bar',color='orange')
data['totals.transactions'].value_counts()
data['totals.transactions']=data['totals.transactions'].fillna('0')
data['totals.transactions']=data['totals.transactions'].astype('Int64')
plt.figure(figsize=(10,5))
#data.groupby('totals.transactions')['totals.transactionRevenue'].mean().plot(kind='bar')
total_trans= pd.pivot_table(data=data,index=['totals.transactions','channelGrouping'],values='totals.transactionRevenue',aggfunc='mean').reset_index()
sns.barplot(data=total_trans,x='totals.transactions',y='totals.transactionRevenue',hue='channelGrouping',dodge=False)
plt.legend(loc='upper right')
plt.show();
visit_trans=pd.pivot_table(data=data, index='channelGrouping',values='totals.transactions',aggfunc='count').reset_index()
visit_trans=visit_trans.sort_values(by='totals.transactions',ascending=False)
sns.barplot(data=visit_trans,x='channelGrouping',y='totals.transactions',color='orange')
plt.xticks(rotation=90)
plt.show();
#visit_trans
visit_trans1=pd.pivot_table(data=data, index='totals.transactions',values='totals.transactionRevenue',aggfunc='mean').reset_index()
sns.barplot(data=visit_trans1,x='totals.transactions',y='totals.transactionRevenue',color='orange')
# Traffic Source :
# Revenue generated through the adContent, is Paid Search.

data['trafficSource.adContent']=data['trafficSource.adContent'].fillna('No Add Content')
plt.figure(figsize=(16,5))
plt.subplots_adjust(hspace=5)
plt.subplot(1,2,1)

add_rev=pd.DataFrame(data.groupby('trafficSource.adContent')['totals.transactionRevenue'].mean()).reset_index()
add_rev=add_rev.sort_values(by='totals.transactionRevenue',ascending=False)
add_rev1=add_rev[add_rev['totals.transactionRevenue']>0]
sns.barplot(data=add_rev1,x='trafficSource.adContent',y='totals.transactionRevenue',color='blue')
plt.xticks(rotation=90)
plt.subplot(1,2,2)
add_chan_rev=pd.DataFrame(data.groupby(['trafficSource.adContent','channelGrouping'])['totals.transactionRevenue'].mean()).reset_index()
add_chan_rev=add_chan_rev.sort_values(by='totals.transactionRevenue',ascending=False)
add_chan_rev1=add_chan_rev[add_chan_rev['totals.transactionRevenue']>0]
sns.barplot(data=add_chan_rev1,x='trafficSource.adContent',y='totals.transactionRevenue',hue='channelGrouping',dodge=False)
plt.xticks(rotation=90)
plt.legend(loc='upper right')
plt.show();


data['trafficSource.adwordsClickInfo.adNetworkType'].value_counts()/data.shape[0]
data.groupby(['trafficSource.adwordsClickInfo.adNetworkType','channelGrouping'])['totals.transactionRevenue'].mean()

data['trafficSource.adwordsClickInfo.adNetworkType']=data['trafficSource.adwordsClickInfo.adNetworkType'].fillna('No Network')
del data['trafficSource.adwordsClickInfo.gclId']
data['trafficSource.adwordsClickInfo.adNetworkType'].value_counts()
data['trafficSource.adwordsClickInfo.page']=data['trafficSource.adwordsClickInfo.page'].fillna('99')
# Top slot ADD, have more impact on Revenue
data['trafficSource.adwordsClickInfo.slot'].value_counts()
data['trafficSource.adwordsClickInfo.slot'].isnull().sum()
data.groupby('trafficSource.adwordsClickInfo.slot')['totals.transactionRevenue'].mean().plot(kind='bar')
data['trafficSource.adwordsClickInfo.slot']=data['trafficSource.adwordsClickInfo.slot'].fillna('No Add Slot')
#data['trafficSource.keyword'].value_counts()
#data['trafficSource.keyword'].isnull().sum()
key_word=pd.DataFrame(data.groupby(['trafficSource.keyword'])['totals.transactionRevenue'].mean()).reset_index()
key_word=key_word.sort_values(by='totals.transactionRevenue',ascending=False)
#del data['trafficSource.keyword']
plt.figure(figsize=(10,5))
data['trafficSource.medium'].isnull().sum()
med=pd.DataFrame(data.groupby(['trafficSource.medium','channelGrouping'])['totals.transactionRevenue'].mean()).reset_index()
med=med.sort_values(by='totals.transactionRevenue',ascending=False)
sns.barplot(data=med,x='trafficSource.medium',y='totals.transactionRevenue',hue='channelGrouping',dodge=False)
plt.legend(loc='upper right')
data['trafficSource.referralPath']=data['trafficSource.referralPath'].fillna('No Reference Path')
#data['trafficSource.source'].value_counts()
#data.isnull().sum()
#del data['trafficSource.adContent']
def process_device_b(data1):
    i = str(data1)
    if '_' in i:
        return 'other'
    elif '.' in i:
        return 'other'
    elif len(i)>25:
        return 'other'
    else:
        return i
    
data['device.browser']=data['device.browser'].apply(process_device_b)
from nltk.stem import PorterStemmer

import re

def key_word(key):
    low_key=str(key).lower()
    #low_key=stemmer.stem(low_key)
    if 'goo' in low_key or 'google' in low_key or 'googl' in low_key or 'stor' in low_key:
        return 'google'
    if 'merchandi' in low_key:
        return 'merchandise'
    if 'youtube' in low_key or 'yo' in low_key or 'yuotub' in low_key or 'yuo tub' in low_key or 'tub' in low_key or 'yutob' in low_key:
        return 'youtube'
    if bool(re.search(r'\d', low_key)):
        return 'other'
    if 'tshirt' in low_key or 't shirt' in low_key or 't-shirt' in low_key:
        return 'tshirt'
    if 'shirts' in low_key or 'shirt' in low_key:
        return 'shirts'
    if 'lamp' in low_key or 'lamps' in low_key:
        return 'lamps'
    if 'apparel' in low_key:
        return 'apparel'
    if len(low_key)>28:
        return 'other'
    if 'note' in low_key:
        return 'notebook'
    if 'android' in low_key:
        return 'android'
    else:
        return low_key


data['trafficSource.keyword']=data['trafficSource.keyword'].apply(key_word)    

def traffic_feature(source_value):
    if 'google' in source_value:
        return 'google'
    elif 'yahoo' in source_value:
        return 'yahoo'
    elif 'facebook' in source_value:
        return 'facebook'
    elif 'youtube' in source_value:
        return 'youtube'
    elif 'pinterest' in source_value:
        return 'pinterest'
    elif 'baidu' in source_value:
        return 'baidu'
    elif 'reddit' in source_value:
        return 'reddit'
    elif 'wow' in source_value:
        return 'wow'
    elif len(source_value)>20:
        return 'other'
    elif 'bing' in source_value:
        return 'bing'
    else:
        return source_value

data['trafficSource.source']=data['trafficSource.source'].apply(traffic_feature)

data.head(5)
#data['device.browser'].value_counts()
# About 54 percent of the values are not available in dataset

len(data[data['geoNetwork.metro']=='not available in demo dataset'])/data.shape[0]*100
#device.operatingSystem
data.groupby(['geoNetwork.networkDomain','channelGrouping'])['channelGrouping'].count()
drop_cols = ['trafficSource.referralPath', 'trafficSource.adContent', 'trafficSource.adwordsClickInfo.slot', 'trafficSource.adwordsClickInfo.page',
            'trafficSource.adwordsClickInfo.adNetworkType','visitStartTime','device.isMobile']
#data.drop(['visitStartTime','device.isMobile'])
data.head(10) #trafficSource.keyword
cols=['channelGrouping','year', 'month', 'day', 'weekday','device.browser', 'totals.hits','totals.pageviews','totals.sessionQualityDim',
'device.deviceCategory','device.operatingSystem', 'geoNetwork.city', 'geoNetwork.continent','totals.timeOnSite',
'geoNetwork.country', 'geoNetwork.metro', 'geoNetwork.networkDomain','trafficSource.adwordsClickInfo.slot','geoNetwork.region', 'geoNetwork.subContinent',
'totals.totalTransactionRevenue','totals.transactions','trafficSource.campaign','trafficSource.keyword','trafficSource.medium','trafficSource.source','set','totals.transactionRevenue']

data=data[cols]
data.head(5)
data.shape
from sklearn.preprocessing import LabelEncoder
data.head(5)
for i, t in data.dtypes.iteritems():
    if t == object:
        data[i] = pd.factorize(data[i])[0]
# spliting back the dataset into train and test set
data.shape
train_df=data[data['set']==0]
test_df=data[data['set']==1]
del train_df['set']
train_df.shape
del test_df['set']
test_df.shape
train_x=train_df.iloc[:,:-1]
train_y=train_df.iloc[:,-1]
test_x=test_df.iloc[:,:-1]
test_y=test_df.iloc[:,-1]

train_x.head(5)
param = { 'objective':'regression',
         'max_depth': -1,
         'learning_rate':0.005,
         "min_child_samples":40,
         "boosting":"gbdt",
         "feature_fraction":0.8,
         "bagging_freq":1,
         "bagging_fraction":0.8 ,
         "bagging_seed": 3,
         "metric": 'rmse',
         "lambda_l1": 1,
         'lambda_l2': 1,
         "verbosity": -1}
train_y = train_y.apply(lambda x: np.log(x) if x > 0 else x)
folds = KFold(n_splits=5, shuffle=True, random_state=15)
oof = np.zeros(len(train_df))
predictions = np.zeros(len(test_df))
start = time.time()
features = list(train_df.columns)
feature_importance_df = pd.DataFrame()

for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_x.values, train_y.values)):
    trn_data = lgb.Dataset(train_x.iloc[trn_idx], label=train_y.iloc[trn_idx])
    val_data = lgb.Dataset(train_x.iloc[val_idx], label=train_y.iloc[val_idx])
    
    num_round = 10000
    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500 ,early_stopping_rounds = 500)
    oof[val_idx] = clf.predict(train_x.iloc[val_idx].values, num_iteration=clf.best_iteration)
    
    fold_importance_df = pd.Series()
    fold_importance_df["feature"] = features
    fold_importance_df["importance"] = clf.feature_importance()
    fold_importance_df["fold"] = fold_ + 1
    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)
    
    predictions += clf.predict(test_x.values, num_iteration=clf.best_iteration) / folds.n_splits
print("CV score: {:<8.5f}".format(mean_squared_error(oof, train_y)**0.5))
submission = pd.DataFrame()

submission['fullVisitorId'] = test['fullVisitorId']
submission['PredictedLogRevenue'] = predictions

submit_file = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()
submit_file.to_csv('submit_group.csv',index=False)
