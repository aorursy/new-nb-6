# -*- coding: utf-8 -*-

"""

Created on Wed Aug  2 12:40:13 2017

This notebook is for parameter tuning of XGBoost

@author: prasun.mishra

"""



#Import libraries:

import pandas as pd

import numpy as np

from xgboost.sklearn import XGBClassifier

from sklearn import cross_validation, metrics   #Additional scklearn functions

from sklearn.grid_search import GridSearchCV   #Perforing grid search

from sklearn.preprocessing import LabelEncoder

import matplotlib.pylab as plt


from matplotlib.pylab import rcParams

rcParams['figure.figsize'] = 12, 4





##################################################



print("\nReading data :")

prop = pd.read_csv('../input/properties_2016.csv')

train_2016 = pd.read_csv("../input/train_2016_v2.csv", parse_dates=["transactiondate"])

sample = pd.read_csv('../input/sample_submission.csv')



print('\nBinding to float32')



for c, dtype in zip(prop.columns, prop.dtypes):

    if dtype == np.float64:

        prop[c] = prop[c].astype(np.float32)

        

print('\nFitting Label Encoder on prop')

for c in prop.columns:

    prop[c]=prop[c].fillna(-1)

    if prop[c].dtype == 'object':

        lbl = LabelEncoder()

        lbl.fit(list(prop[c].values))

        prop[c] = lbl.transform(list(prop[c].values))



###################################################        



train = train_2016.merge(prop, how='left', on='parcelid')

train.fillna(train.mean(),inplace = True)

train = train.drop(['parcelid','transactiondate', 'propertyzoningdesc', 'propertycountylandusecode','fireplacecnt', 'fireplaceflag'], axis=1)

target = 'logerror'

IDcol = 'ParcelId'

##################################################



def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):

    print('*** Here in modelfit ******* Point 1')

    if useTrainCV:

        print('*** Here in modelfit ******* Point 2')            

        xgb_param = alg.get_xgb_params()

        print('*** Here in modelfit ******* Point 3')

        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)

        print('*** Here in modelfit ******* Point 4')

        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,

            metrics='mae', early_stopping_rounds=early_stopping_rounds, verbose_eval=10)

        alg.set_params(n_estimators=cvresult.shape[0])

        print('*** Here in modelfit ******* Point 5')

    

    #Fit the algorithm on the data

    print('*** Here in modelfit ******* Point 5.5')

    print ("Here predictors are:",predictors )

    alg.fit(dtrain[predictors], dtrain['logerror'],eval_metric='mae')

    #alg.fit(dtrain[predictors], dtrain['logerror'])

    print('*** Here in modelfit ******* Point 6')

        

    #Predict training set:

    dtrain_predictions = alg.predict(dtrain[predictors])

    print('*** Here in modelfit ******* Point 7')

    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]

    print('*** Here in modelfit ******* Point 8')

        

    #Print model report:

    print ("\nModel Report")

    print ("\nAccuracy : %.4g" % metrics.accuracy_score(dtrain['logerror'].values, dtrain_predictions))

    print ("\nMAE Score (Train): %f" % metrics.mean_absolute_error(dtrain['logerror'], dtrain_predprob))

                    

    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)

    print('*** Here in modelfit ******* Point 9')

    feat_imp.plot(kind='bar', title='Feature Importances')

    print('*** Here in modelfit ******* Point 10')

    plt.ylabel('Feature Importance Score')

    print('*** Here in modelfit ******* Point 11')

    

#Choose all predictors except target & IDcols

print('****** This is point 1')

predictors = [x for x in train.columns if x not in [target, IDcol]]

xgb1 = XGBClassifier(

 learning_rate =0.1,

 n_estimators=1000,

 max_depth=5,

 min_child_weight=1,

 gamma=0,

 subsample=0.8,

 colsample_bytree=0.8,

 objective= 'reg:linear',

 nthread=4,

 scale_pos_weight=1,

 seed=27)

modelfit(xgb1, train, predictors)



print('****** This is point 2')



param_test1 = {

 'max_depth':range(3,10,2),

 'min_child_weight':range(1,6,2)

}



print('****** This is point 3')

gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,

 min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,

 objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27), 

 param_grid = param_test1, scoring='mae',n_jobs=4,iid=False, cv=5)



print('****** This is point 4')

gsearch1.fit(train[predictors],train[target])

print('****** This is point 5')

gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_

print('****** This is point 6')





param_test2 = {

 'max_depth':[4,5,6],

 'min_child_weight':[4,5,6]

}

gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,

 min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,

 objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=27), 

 param_grid = param_test2, scoring='mae',n_jobs=4,iid=False, cv=5)

gsearch2.fit(train[predictors],train[target])

gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_



param_test2b = {

 'min_child_weight':[6,8,10,12]

}

gsearch2b = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=4,

 min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,

 objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=27), 

 param_grid = param_test2b, scoring='mae',n_jobs=4,iid=False, cv=5)

gsearch2b.fit(train[predictors],train[target])



modelfit(gsearch2b.best_estimator_, train, predictors)

gsearch2b.grid_scores_, gsearch2b.best_params_, gsearch2b.best_score_





param_test3 = {

 'gamma':[i/10.0 for i in range(0,5)]

}

gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4,

 min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,

 objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=27), 

 param_grid = param_test3, scoring='mae',n_jobs=4,iid=False, cv=5)

gsearch3.fit(train[predictors],train[target])

gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_



xgb2 = XGBClassifier(

 learning_rate =0.1,

 n_estimators=1000,

 max_depth=4,

 min_child_weight=6,

 gamma=0,

 subsample=0.8,

 colsample_bytree=0.8,

 objective= 'reg:linear',

 nthread=4,

 scale_pos_weight=1,

 seed=27)

modelfit(xgb2, train, predictors)



param_test4 = {

 'subsample':[i/10.0 for i in range(6,10)],

 'colsample_bytree':[i/10.0 for i in range(6,10)]

}

gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,

 min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,

 objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=27), 

 param_grid = param_test4, scoring='mae',n_jobs=4,iid=False, cv=5)

gsearch4.fit(train[predictors],train[target])

gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_

param_test5 = {

 'subsample':[i/100.0 for i in range(75,90,5)],

 'colsample_bytree':[i/100.0 for i in range(75,90,5)]

}

gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,

 min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,

 objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=27), 

 param_grid = param_test5, scoring='mae',n_jobs=4,iid=False, cv=5)

gsearch5.fit(train[predictors],train[target])





param_test6 = {

 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]

}

gsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,

 min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,

 objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=27), 

 param_grid = param_test6, scoring='mae',n_jobs=4,iid=False, cv=5)

gsearch6.fit(train[predictors],train[target])

gsearch6.grid_scores_, gsearch6.best_params_, gsearch6.best_score_



param_test7 = {

 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]

}

gsearch7 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,

 min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,

 objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=27), 

 param_grid = param_test7, scoring='mae',n_jobs=4,iid=False, cv=5)

gsearch7.fit(train[predictors],train[target])

gsearch7.grid_scores_, gsearch7.best_params_, gsearch7.best_score_





xgb3 = XGBClassifier(

 learning_rate =0.1,

 n_estimators=1000,

 max_depth=4,

 min_child_weight=6,

 gamma=0,

 subsample=0.8,

 colsample_bytree=0.8,

 reg_alpha=0.005,

 objective= 'reg:linear',

 nthread=4,

 scale_pos_weight=1,

 seed=27)

modelfit(xgb3, train, predictors)



xgb4 = XGBClassifier(

 learning_rate =0.01,

 n_estimators=5000,

 max_depth=4,

 min_child_weight=6,

 gamma=0,

 subsample=0.8,

 colsample_bytree=0.8,

 reg_alpha=0.005,

 objective= 'reg:linear',

 nthread=4,

 scale_pos_weight=1,

 seed=27)

modelfit(xgb4, train, predictors)




