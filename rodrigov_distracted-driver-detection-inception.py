import numpy as np 

import pandas as pd 

import os

from keras import layers

from keras import models

from keras.utils import to_categorical

import matplotlib.pyplot as plt

from os import listdir, makedirs

from keras.callbacks import ModelCheckpoint

from keras.preprocessing.image import ImageDataGenerator

from keras.applications import VGG16, ResNet50, VGG19, InceptionV3, MobileNetV2

from keras.models import Sequential

from keras.layers import Dense, Flatten, Dropout, Conv2D, Activation, MaxPooling2D, BatchNormalization

from keras import optimizers, regularizers

from keras.optimizers import SGD

from glob import glob

import cv2

import glob

from keras import backend as K

import numpy as np 

import pandas as pd 

import os

from keras import layers

from keras import models, Sequential

from keras.utils import to_categorical

import matplotlib.pyplot as plt

from os import listdir, makedirs

from keras.callbacks import ModelCheckpoint

from keras.preprocessing.image import ImageDataGenerator

from keras.applications import VGG16, ResNet50, VGG19, InceptionV3

from keras.models import Sequential

from keras.layers import Dense, Flatten, Dropout

from keras.preprocessing.image import load_img

from keras import optimizers, regularizers

from keras.optimizers import SGD

from glob import glob

import cv2

from keras.callbacks import EarlyStopping, Callback

from keras.preprocessing import image

from keras.applications.inception_v3 import preprocess_input

from keras.applications.imagenet_utils import preprocess_input, decode_predictions

from keras.models import load_model

import h5py

from PIL import Image

from sklearn.model_selection import train_test_split



print(os.listdir("../input"))

data_dir = '../input/'



RESOLUTION = 150

BATCH_SIZE=64



train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.3,)

val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.3)



train_generator = train_datagen.flow_from_directory(

        "../input/train/",

        target_size=(160, 120),

        batch_size=BATCH_SIZE,

        class_mode='categorical', subset="training", color_mode='grayscale')



val_generator = val_datagen.flow_from_directory(

        "../input/train/",

        target_size=(160, 120),

        batch_size=BATCH_SIZE,

        class_mode='categorical', subset="validation",  color_mode='grayscale')
def read_image(path):

    image = cv2.imread(path, cv2.IMREAD_COLOR)

    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    return image



labels = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']



col = {'c0': 'safe driving',

'c1': 'texting - right',

'c2': 'talking on the phone - right',

'c3': 'texting - left',

'c4': 'talking on the phone - left',

'c5':'operating the radio',

'c6': 'drinking',

'c7': 'reaching behind',

'c8': 'hair and makeup',

'c9': 'talking to passenger'}
for label in labels:

    f, ax = plt.subplots(figsize=(12, 10))

    files = glob('{}/train/{}/*.jpg'.format(data_dir, label))

    

    print('\t\t\t\t# {} : {}'.format(label, col[label]))

    for x in range(9):

        plt.subplot(3, 3, x+1)

        image = read_image(files[x])

        plt.imshow(image)

        plt.axis('off')

    plt.show()

    


def create_model():

    model = Sequential()

    # Use Batch Normalization for every conv and dense layers

    model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu', input_shape = (160, 120, 1)))

    model.add(BatchNormalization())

    model.add(Dropout(0.25))

    model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))

    model.add(BatchNormalization())

    model.add(MaxPooling2D(pool_size=(2,2)))

    model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu'))

    model.add(BatchNormalization())

    model.add(Dropout(0.25))

    model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu'))

    model.add(BatchNormalization())

    model.add(MaxPooling2D(pool_size=(2,2)))

    model.add(Flatten())

    model.add(Dense(64, activation = 'relu'))

    model.add(BatchNormalization())

    model.add(Dropout(0.33))

    model.add(Dense(32, activation = 'relu'))

    model.add(BatchNormalization())

    model.add(Dense(10, activation = 'softmax')) 

    model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])

    return model
model = create_model()
def train(model, filepath):



    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')

    callbacks_list = [checkpoint]

    

    n_train = 15702

    batch_size = 300

    n_valid = 6722

    history = model.fit_generator(

           train_generator,

           steps_per_epoch=n_train//batch_size,

           epochs=10,

           validation_data=val_generator,

           validation_steps=n_valid//batch_size,  callbacks=callbacks_list)

    

    

    # Plot

    acc = history.history['acc']

    val_acc = history.history['val_acc']

    loss = history.history['loss']

    val_loss = history.history['val_loss']

    epochs = range(1, len(acc) + 1)

    plt.plot(epochs, acc, 'bo', label='Training acc')

    plt.plot(epochs, val_acc, 'b', label='Validation acc')

    plt.title('Training and validation accuracy')

    plt.legend()

    plt.figure()

    plt.plot(epochs, loss, 'bo', label='Training loss')

    plt.plot(epochs, val_loss, 'b', label='Validation loss')

    plt.title('Training and validation loss')

    plt.legend()

    plt.show()



    return model
model = train(model, "weights_best.h5")